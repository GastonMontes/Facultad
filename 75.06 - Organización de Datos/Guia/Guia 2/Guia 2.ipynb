{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1) Como framework de cluster computing, Apache Spark provee una abstracción llamada RDDs (Resilent Distributed Datasets). De los siguientes opciones, solo una es correcta en relación los RDDs.\t \n",
    " \t\t a) Son la principal abstracción de datos usada en Hadoop.\n",
    "         b) Son particularmente lentos para tareas de Interactive Mining por las operaciones de entrada-salida que requieren.\n",
    "         c) Son reconstruidos automáticamente por Spark frente a un fallo en una maquina del cluster o si un cierto job se encuentra funcionando lento.\n",
    "         d) Frente a un fallo, pueden quedar en estado inconsistente.\n",
    "         \n",
    "RTA: la opción elegida es la opción c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2) Dado el siguiente código de pyspark e ignorando la definicion de pairRDD -- print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect() print pairRDD.reduceByKey(add).collect() -- Obtenemos la siguiente salida -- [('a', 3), ('b', 1)] [('a', 3), ('b', 1)] -- ¿Como debería estar definida pairRDD para obtener ese resultado?\n",
    "\n",
    "a) \tpairRDD = [('a', 1), ('a', 1), ('a', 1), ('b', 1)]\n",
    "b) \tpairRDD = sc.parallelize([('a', 2), ('a', 1), ('b', 1)])\n",
    "c) \tpairRDD = sc.parallelize([('b', 1), ('b', 1), ('b', 1), ('a', 1)]).repartition(2)\n",
    "d) \tpairRDD = sc.parallelize([('a', 1), ('a', 2), ('b', 2)])\n",
    "\n",
    "RTA: la opción elegida es la opción b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import csv\n",
    "\n",
    "try: \n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 1), ('a', 1), ('b', 1)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pruebo con el rdd del punto a).\n",
    "pairRDD = [('a', 1), ('a', 1), ('a', 1), ('b', 1)]\n",
    "print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect() \n",
    "print pairRDD.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 3), ('b', 1)]\n",
      "[('a', 3), ('b', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('a', 1), ('b', 1)]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pruebo con el rdd del punto b).\n",
    "pairRDD = sc.parallelize([('a', 2), ('a', 1), ('b', 1)])\n",
    "print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect() \n",
    "print pairRDD.reduceByKey(lambda x, y: x + y).collect()\n",
    "pairRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 3)]\n",
      "[('a', 1), ('b', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Pruebo con el rdd del punto c).\n",
    "pairRDD = sc.parallelize([('b', 1), ('b', 1), ('b', 1), ('a', 1)]).repartition(2)\n",
    "print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect() \n",
    "print pairRDD.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 3), ('b', 2)]\n",
      "[('a', 3), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Pruebo con el rdd del punto d).\n",
    "pairRDD = sc.parallelize([('a', 1), ('a', 2), ('b', 2)])\n",
    "print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect() \n",
    "print pairRDD.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Tengo un RDD con los siguientes campos: padron, apellido_nombre, fecha_nacimiento, fecha_ingreso, codigo_carrera.\n",
    "\n",
    "Indicar cuales de los siguientes es correcto:\n",
    "\n",
    "a) \trdd.filter(lambda row: row.codigo_carrera == '10').filter(lambda row: row.fecha_ingreso > '2016-01-01').takeSample(false, 10) obtiene la cantidad de alumnos inscriptos en la carrera 10 a partir del 2016.\n",
    "b) \trdd.filter(lambda row: row.codigo_carrera == '10').filter(lambda row: row.fecha_ingreso > '2016-01-01').map(lambda row: (len(row.apellido_nombre), 1).reduceByKey(lambda a, b: a+b).collect() obtiene la cantidad de alumnos por longitud de apellido y nombre para los alumnos de la carrera 10.\n",
    "c) \trdd.filter(lambda row: row.codigo_carrera == '10').filter(lambda row: row.fecha_ingreso > '2016-01-01').map(lambda row: (len(row.apellido_nombre), 1).reduceByKey(lambda a, b: a+b).collect() obtiene los apellidos de los alumnos que hayan ingresado despues del 2016-01-01 a la carrera 10.\n",
    " d) \trdd.map(lambda row: (row.fecha_ingreso.year, 1)).reduceByKey(lambda a, b: a+b).collect() obtiene la cantidad de inscriptos por año."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizo el punto a).\n",
    "## Obtiene la cantidad de alumnos inscriptos en la carrera 10 a partir del 2016.\n",
    "\n",
    "### Con este filter me quedo con los alumnos de la carrera 10.\n",
    "rdd.filter(lambda row: row[3] == \"10\")\n",
    "\n",
    "### En este filter me quedo con los alumnos anotados a partiro del dia 2016-01-02.\n",
    ".filter(lambda row: row.fecha_ingreso > '2016-01-01')\n",
    "\n",
    "## RTA: FALSO - No contempla los alumnos anotados el día 2016-01-01 que también forma parte del año 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizo el punto b).\n",
    "## Obtiene la cantidad de alumnos por longitud de apellido y nombre para los alumnos de la carrera 10.\n",
    "### Con este filter me quedo con los alumnos de la carrera 10.\n",
    "rdd.filter(lambda row: row.codigo_carrera == '10')\n",
    "\n",
    "### En este filter me quedo con los alumnos anotados a partiro del dia 2016-01-02.\n",
    ".filter(lambda row: row.fecha_ingreso > '2016-01-01')\n",
    "\n",
    "### Este map genera una tupla con la longitud de campo \"apellido_nombre\" y un 1: (longitud, 1).\n",
    ".map(lambda row: (len(row.apellido_nombre), 1)\n",
    "\n",
    "### Hace un reduce por longitud y suma la cantidad de veces que aparece la misma longitud: (longitud, veces)\n",
    ".reduceByKey(lambda a, b: a+b).collect() \n",
    "\n",
    "## RTA: FALSO - No se consideran los alumnos totales, sino aquellos que ingresaron a partir del día 2016-01-02.\n",
    "                                                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizo el punto c).\n",
    "## Obtiene los apellidos de los alumnos que hayan ingresado despues del 2016-01-01 a la carrera 10.\n",
    "\n",
    "### Es la misma instrucción que en el punto b).\n",
    "\n",
    "## RTA: Verdadero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizo el punto d).\n",
    "## Obtiene la cantidad de inscriptos por año.\n",
    "\n",
    "### Esta instrucción falla ya que no existe row.fecha_ingreso.year (En ningún momento la crea.)\n",
    "rdd.map(lambda row: (row.fecha_ingreso.year, 1))\n",
    ".reduceByKey(lambda a, b: a+b)\n",
    ".collect()\n",
    "\n",
    "## RTA: FALSA.\n",
    "\n",
    "# ESTABA MAL LA RESPUESTA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Tenemos un RDD con números enteros llamado intergerListRDD al cual le aplicamos las siguientes acciones. -- integersListRDD.reduce(lambda a, b: a - b) integersListRDD.repartition(6).reduce(lambda a, b: a - b) -- Que podemos decir los resultados de la aplicar el reduce por un lado y de aplicar del repartition y el reduce.\t \n",
    " \t\t a) \tLos resultados serán iguales dado que no hay que tener consideraciones especiales al aplicar una operacion de reduce sobre los datos.\n",
    " \t\t b) \tLos resultados serán distintos, dado que el reduce es aplicado a nivel de partición y luego nuevamente para agregar resultados de particiones, por lo que se requiere que la función planteada de sea conmutativa y asociativa\n",
    " \t\t c) \tLos resultados serán distintos dado que la operacion de repartition crea un nuevo RDD con el exacto número de particiones indicado (en este caso 6).\n",
    " \t\t d) \tLos resultados seran iguales dado que la operacion de resta es no conmutativa y asociativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5563370559\n",
      "5987639305\n"
     ]
    }
   ],
   "source": [
    "integersListRDD = sc.parallelize([4325543, 32452345, 98567456, 67856, 34523, 3845, 56867, 2134768, 4366, 34435, 546756, 567546, 5674567, 89780, 80768965, 34534, 896789, 56678, 456435, 678567, 56745642, 324234, 678567, 4356, 4564, 435645, 4356435, 4563456, 456768, 32452345, 123143, 786554, 324523, 69787, 32423, 68976, 3252, 768567, 325321, 5674564334, 3245234, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 34345, 234 ,467, 345, 435, 567, 234, 768, 634, 58543, 3245, 3453, 345])\n",
    "print integersListRDD.reduce(lambda a, b: a - b)\n",
    "print integersListRDD.repartition(6).reduce(lambda a, b: a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizo las posibles respuestas\n",
    "\n",
    "## a) Los resultados serán iguales dado que no hay que tener consideraciones especiales al aplicar una operacion de reduce sobre los datos.\n",
    "FALSA - Las funciones que se aplican mediante el reduce deben ser conmutativas y asociativas. En este caso si los datos estan todos en la misma particion difiere de si los datos estan distribuidos en distintas particiones, por lo que la operación de resta no va a dar lo mismo.\n",
    "\n",
    "## b) Los resultados serán distintos, dado que el reduce es aplicado a nivel de partición y luego nuevamente para agregar resultados de particiones, por lo que se requiere que la función planteada de sea conmutativa y asociativa.\n",
    "VERDADERA.\n",
    "\n",
    "## c) Los resultados serán distintos dado que la operacion de repartition crea un nuevo RDD con el exacto número de particiones indicado (en este caso 6).\n",
    "FALSA - No se crean nuevos RDDs, simplemente se distribuye sus datos en diferentes particiones.\n",
    "\n",
    "## d) Los resultados seran iguales dado que la operacion de resta es no conmutativa y asociativa.\n",
    "FALSA - Para que el reduce dé resultados iguales, la operación a aplicar debe ser conmutativa y asociativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Archivo.txt:\n",
    "\n",
    "datos spark guia\n",
    "\n",
    "spark hadoop \n",
    "\n",
    "guia datos spark gradience\n",
    "\n",
    "Corremos:\n",
    "\n",
    "sc.textFile(\"Archivo.txt\")\n",
    ".flatMap(lambda a: a.split())\n",
    ".map(lambda a: (a, 1))\n",
    ".reduceByKey(lambda a, b: a+b)\n",
    ".takeOrdered(3, key = lambda x: -x[1])\n",
    "\n",
    "Cual de las respuestas es correcta?\n",
    " \n",
    " \t\t a) \tEl flatMap devuelve un registro para cada palabra en el archivo.\n",
    " \t\t b) \tEl takeOrdered obtiene las 3 primeras palabras ordenadas alfabeticamente.\n",
    " \t\t c) \tMap no puede funcionar desde un archivo.\n",
    " \t\t d) \tNo es posible hacer un takeOrdered porque hay un reduceByKey antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark', 3), (u'datos', 2), (u'guia', 2)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archivo = sc.textFile(\"Data/Archivo.txt\")\n",
    "\n",
    "# Creo un registro por cada palabra.\n",
    "archivo = archivo.flatMap(lambda a: a.split())\n",
    "\n",
    "# Por cada palabra creo un registro que sea una tupla del estilo: (palabra, 1)\n",
    "archivo = archivo.map(lambda a: (a, 1))\n",
    "\n",
    "# Con el reduce obtengo un registro por palabra con la suma de sus apariciones: (palabra, suma)\n",
    "archivo = archivo.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# El take orderen me devuelve los primeros 3 registros cuyo valor de la suma sea mayor.\n",
    "archivo.takeOrdered(3, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizo las opciones:\n",
    "## a) El flatMap devuelve un registro para cada palabra en el archivo.\n",
    "FALSO - El flatMap aplica la función a todos los elementos del RDD y luego hace un flat de los resultdos.\n",
    "## b) El takeOrdered obtiene las 3 primeras palabras ordenadas alfabeticamente.\n",
    "FALSO - Obtiene las 3 palabras con mayor frecuencia.\n",
    "## c) Map no puede funcionar desde un archivo.\n",
    "VERDADERO - Seleccionada por descarte. No entendí bien a que se refería.\n",
    "## d) No es posible hacer un takeOrdered porque hay un reduceByKey antes.\n",
    "FALSO - El take ordered se puede hacer sobre cualquier rdd sobre el cual tengamos alguna propiedad para comparar.\n",
    "\n",
    "# ESTABA MAL LA RESPUESTA! ERA LA a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Archivo.txt:\n",
    "\n",
    "datos spark guia\n",
    "\n",
    "spark hadoop \n",
    "\n",
    "guia datos spark gradience\n",
    "\n",
    "Corremos:\n",
    "\n",
    "sc.textFile(\"Archivo.txt\")\n",
    ".flatMap(lambda a: a.split())\n",
    ".map(lambda a: (a, 1))\n",
    ".reduceByKey(lambda a, b: a+b)\n",
    ".takeOrdered(3, key = lambda x: -x[1])\n",
    "\n",
    "Cual de las respuestas es correcta?\n",
    " \n",
    " \t\t a) \tEl resultado solo tiene 3 registros.\n",
    " \t\t b) \tEl resultado es la tercer palabra que más apariciones tiene.\n",
    " \t\t c) \tEl resultado son las 3 palabras más largas.\n",
    " \t\t d) \tCon flatMap obtengo un registro para cada letra del archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizo las opciones.\n",
    "La instrucción es la misma que en el punto 5).\n",
    "\n",
    "## a) El resultado solo tiene 3 registros.\n",
    "VERDADERO - En el take ordered le pasamos el parametro 3 para que nos devuelva las 3 palabras con mas frecuencia.\n",
    "\n",
    "## b) El resultado es la tercer palabra que más apariciones tiene.\n",
    "FALSO - El resultado son las 3 palabras con mas frecuencia.\n",
    "\n",
    "## c) El resultado son las 3 palabras más largas.\n",
    "FALSO - El resultado son las 3 palabras con mas frecuencia.\n",
    "\n",
    "## d) El flatMap devuelve un registro para cada palabra en el archivo.\n",
    "FALSO - El flatMap aplica la función a todos los elementos del RDD y luego hace un flat de los resultdos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'datos spark guia', u'spark hadoop ', u'guia datos spark gradience']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archivo = sc.textFile(\"Data/Archivo.txt\")\n",
    "archivo.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
