{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "try: \n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  \tTengo un RDD con los siguientes campos: padron, apellido_nombre, fecha_nacimiento, fecha_ingreso, codigo_carrera.\n",
    "\n",
    "### Indicar cuales de los siguientes es correcto:\t \n",
    "#### \t\t a) \trdd.filter(lambda row: row.fecha_nacimiento < '1970-01-01').filter(lambda row: row.fecha_ingreso > '2010-01-01').map(lambda row: (date.today().year - row.fecha_nacimiento.year, 1)).reduce(lambda a, b: (a[0] + b[0], a[1] + b[1])) obtiene la edad de los alumnos con fecha de nacimiento anterior al 1970 y que hayan ingresado despues del 1 de Enero de 2010.\n",
    "#### \t\t b) \trdd.filter(lambda row: row.codigo_carrera == '10').filter(lambda row: row.fecha_ingreso > '2016-01-01').takeSample(false, 10) obtiene 10 alumnos al azar que hayan ingresado despues del 2016-01-01 a la carrera 10.\n",
    "#### \t\t c) \trdd.filter(lambda row: row.codigo_carrera == '10').filter(lambda row: row.fecha_ingreso > '2016-01-01').takeSample(false, 10) obtiene los 10 alumnos que se inscribieron en el año 2016 primeros a la carrera 10.\n",
    "#### \t\t d) \trdd.map(lambda row: (row.fecha_ingreso.year, 1)).reduceByKey(lambda a, b: a+b).collect() obtiene la cantidad de inscriptos por día."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(89397, 'Gaston Montes', '1988-10-07', '2005-01-01', '10'),\n",
       " (90244, 'Gonzalo Magallan', '1990-11-17', '2010-07-01', '4'),\n",
       " (89341, 'Mauro Rambosio', '1959-04-23', '2008-01-01', '2'),\n",
       " (91434, 'Angeles Solera', '1932-01-07', '2011-08-01', '5'),\n",
       " (88323, 'Eliana Rodriguez', '1956-05-04', '2017-04-12', '10'),\n",
       " (78966, 'Fabiana Gonzalez', '1956-05-04', '2018-05-13', '10'),\n",
       " (97887, 'Margot Ferreyra', '1956-05-04', '2016-06-14', '10'),\n",
       " (67988, 'Florencia Ceviche', '1956-05-04', '2014-07-15', '10'),\n",
       " (89398, 'Montes Gonzalo', '1986-07-10', '2005-08-01', '12'),\n",
       " (89399, 'Montes Guillermo', '1986-12-26', '2016-03-01', '10'),\n",
       " (89400, 'Montes Gustavo', '1990-10-20', '2016-03-01', '12'),\n",
       " (89401, 'Gutierrez Francisco', '1988-07-10', '2007-03-01', '10'),\n",
       " (89402, 'Moldaini Franco', '1986-07-10', '2005-08-01', '12'),\n",
       " (89403, 'Campo Gabriela', '1986-12-26', '2016-03-01', '10'),\n",
       " (89404, 'Campo Maria Eugenia', '1990-10-20', '2016-03-01', '12'),\n",
       " (89405, 'Astegiano Franco', '1988-07-10', '2007-03-01', '10'),\n",
       " (89406, 'Astegiano Rodrigo', '1986-07-10', '2005-08-01', '12'),\n",
       " (89407, 'Tatin Eloy', '1986-12-26', '2016-03-01', '10'),\n",
       " (89408, 'Serralta Guillermo', '1990-10-20', '2016-03-01', '12'),\n",
       " (89409, 'Serralta Gonzalo', '1988-07-10', '2007-03-01', '10'),\n",
       " (89410, 'Calvino Luciano', '1986-07-10', '2005-08-01', '12'),\n",
       " (89411, 'Salas Nestor', '1986-12-26', '2016-03-01', '10'),\n",
       " (89412, 'Fonseca Julian', '1990-10-20', '2016-03-01', '12'),\n",
       " (89413, 'Barabaschi Walter', '1988-07-10', '2007-03-01', '10'),\n",
       " (89414, 'Gongona Alejandro', '1986-07-10', '2005-08-01', '12'),\n",
       " (89415, 'Munioz Agustina', '1986-12-26', '2016-03-01', '10'),\n",
       " (89416, 'Barcelo Micaela', '1990-10-20', '2016-03-01', '12'),\n",
       " (89417, 'Ferraro Sebastian', '1988-07-10', '2007-03-01', '10'),\n",
       " (89418, 'Ariel Franco', '1986-07-10', '2005-08-01', '12'),\n",
       " (89419, 'Ariel Yanina', '1986-12-26', '2016-03-01', '10'),\n",
       " (89420, 'Minati Alejandro', '1990-10-20', '2016-03-01', '12'),\n",
       " (89421, 'Minati Florencia', '1988-07-10', '2007-03-01', '10'),\n",
       " (89422, 'Castro Rafael', '1986-07-10', '2005-08-01', '12'),\n",
       " (89423, 'Polenta Nicolas', '1986-12-26', '2016-03-01', '10'),\n",
       " (89424, 'Campo Virginia', '1990-10-20', '2016-03-01', '12'),\n",
       " (89425, 'Breque Emilia', '1988-07-10', '2007-03-01', '10'),\n",
       " (89426, 'Dish Paula', '1986-07-10', '2005-08-01', '12'),\n",
       " (89427, 'Montesino Manolo', '1986-12-26', '2016-03-01', '10'),\n",
       " (89428, 'Rodriguez Demetrio', '1990-10-20', '2016-03-01', '12')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creo la data de prueba.\n",
    "from datetime import datetime\n",
    "alumnos = [\n",
    "    (89397, 'Gaston Montes', '1988-10-07', '2005-01-01', '10'),\n",
    "    (90244, 'Gonzalo Magallan', '1990-11-17', '2010-07-01', '4'),\n",
    "    (89341, 'Mauro Rambosio', '1959-04-23', '2008-01-01', '2'),\n",
    "    (91434, 'Angeles Solera', '1932-01-07', '2011-08-01', '5'),\n",
    "    (88323, 'Eliana Rodriguez', '1956-05-04', '2017-04-12', '10'),\n",
    "    (78966, 'Fabiana Gonzalez', '1956-05-04', '2018-05-13', '10'),\n",
    "    (97887, 'Margot Ferreyra', '1956-05-04', '2016-06-14', '10'),\n",
    "    (67988, 'Florencia Ceviche', '1956-05-04', '2014-07-15', '10'),\n",
    "    (89398, \"Montes Gonzalo\", \"1986-07-10\", \"2005-08-01\", \"12\"),\n",
    "    (89399, \"Montes Guillermo\", \"1986-12-26\", \"2016-03-01\", \"10\"),\n",
    "    (89400, \"Montes Gustavo\", \"1990-10-20\", \"2016-03-01\", \"12\"),\n",
    "    (89401, \"Gutierrez Francisco\", \"1988-07-10\", \"2007-03-01\", \"10\"),\n",
    "    (89402, \"Moldaini Franco\", \"1986-07-10\", \"2005-08-01\", \"12\"),\n",
    "    (89403, \"Campo Gabriela\", \"1986-12-26\", \"2016-03-01\", \"10\"),\n",
    "    (89404, \"Campo Maria Eugenia\", \"1990-10-20\", \"2016-03-01\", \"12\"),\n",
    "    (89405, \"Astegiano Franco\", \"1988-07-10\", \"2007-03-01\", \"10\"),\n",
    "    (89406, \"Astegiano Rodrigo\", \"1986-07-10\", \"2005-08-01\", \"12\"),\n",
    "    (89407, \"Tatin Eloy\", \"1986-12-26\", \"2016-03-01\", \"10\"),\n",
    "    (89408, \"Serralta Guillermo\", \"1990-10-20\", \"2016-03-01\", \"12\"),\n",
    "    (89409, \"Serralta Gonzalo\", \"1988-07-10\", \"2007-03-01\", \"10\"),\n",
    "    (89410, \"Calvino Luciano\", \"1986-07-10\", \"2005-08-01\", \"12\"),\n",
    "    (89411, \"Salas Nestor\", \"1986-12-26\", \"2016-03-01\", \"10\"),\n",
    "    (89412, \"Fonseca Julian\", \"1990-10-20\", \"2016-03-01\", \"12\"),\n",
    "    (89413, \"Barabaschi Walter\", \"1988-07-10\", \"2007-03-01\", \"10\"),\n",
    "    (89414, \"Gongona Alejandro\", \"1986-07-10\", \"2005-08-01\", \"12\"),\n",
    "    (89415, \"Munioz Agustina\", \"1986-12-26\", \"2016-03-01\", \"10\"),\n",
    "    (89416, \"Barcelo Micaela\", \"1990-10-20\", \"2016-03-01\", \"12\"),\n",
    "    (89417, \"Ferraro Sebastian\", \"1988-07-10\", \"2007-03-01\", \"10\"),\n",
    "    (89418, \"Ariel Franco\", \"1986-07-10\", \"2005-08-01\", \"12\"),\n",
    "    (89419, \"Ariel Yanina\", \"1986-12-26\", \"2016-03-01\", \"10\"),\n",
    "    (89420, \"Minati Alejandro\", \"1990-10-20\", \"2016-03-01\", \"12\"),\n",
    "    (89421, \"Minati Florencia\", \"1988-07-10\", \"2007-03-01\", \"10\"),\n",
    "    (89422, \"Castro Rafael\", \"1986-07-10\", \"2005-08-01\", \"12\"),\n",
    "    (89423, \"Polenta Nicolas\", \"1986-12-26\", \"2016-03-01\", \"10\"),\n",
    "    (89424, \"Campo Virginia\", \"1990-10-20\", \"2016-03-01\", \"12\"),\n",
    "    (89425, \"Breque Emilia\", \"1988-07-10\", \"2007-03-01\", \"10\"),\n",
    "    (89426, \"Dish Paula\", \"1986-07-10\", \"2005-08-01\", \"12\"),\n",
    "    (89427, \"Montesino Manolo\", \"1986-12-26\", \"2016-03-01\", \"10\"),\n",
    "    (89428, \"Rodriguez Demetrio\", \"1990-10-20\", \"2016-03-01\", \"12\")\n",
    "]\n",
    "\n",
    "alumnosRDD = sc.parallelize(alumnos)\n",
    "alumnosRDD.collect() # Lo hago porque se que son pocos registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-49-ce6f71cbbe79>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-ce6f71cbbe79>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    alumnosRDD.filter(lambda row: row[2] < '1970-01-01') \\ # Filtra los registros cuya fecha de nacimiento sea antes de 1970.\u001b[0m\n\u001b[0m                                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# Resuelvo el punto a).\n",
    "# rdd.filter(lambda row: row.fecha_nacimiento < '1970-01-01')\n",
    "#        .filter(lambda row: row.fecha_ingreso > '2010-01-01')\n",
    "#        .map(lambda row: (date.today().year - row.fecha_nacimiento.year, 1))\n",
    "#        .reduce(lambda a, b: (a[0] + b[0], a[1] + b[1])) \n",
    "# obtiene la edad de los alumnos con fecha de nacimiento anterior al 1970 \n",
    "# y que hayan ingresado despues del 1 de Enero de 2010.\n",
    "\n",
    "import datetime as date\n",
    "alumnosRDD.filter(lambda row: row[2] < '1970-01-01') \\ # Filtra los registros cuya fecha de nacimiento sea antes de 1970.\n",
    "            .filter(lambda row: row[3] > '2010-01-01') \\ # Filtra los registros que ingresaron despues del 1ro del 2010.\n",
    "            .map(lambda row: (date.today().year - row[2].year, 1)) \\ # Mapea el RDD a (edad, 1)\n",
    "            .reduce(lambda a, b: (a[0] + b[0], a[1] + b[1])) # Reduce a una tupla (Suma de edades, Suma de registros)\n",
    "\n",
    "# RTA: Este ejercicio devuelve una tupla (SumaDeEdades, SumaDeRegistros).\n",
    "# Donde SumaDeEdades = La suma de las edades de los registros con fecha de nacimiento anterior a 1970 y\n",
    "# Cuyo ingreso es posterior al 01/01/2010.\n",
    "# Conclusion: FALSO - No obtiene la edad de los alumnos sino la suma de edades de dichos alumnos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-69-132bbed0a98b>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-69-132bbed0a98b>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    alumnosRDD.filter(lambda row: row[4] == '10') \\ # Toma los alumnos solo de la carrera 10.\u001b[0m\n\u001b[0m                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# Resuelvo el punto b).\n",
    "# rdd.filter(lambda row: row.codigo_carrera == '10')\n",
    "#        .filter(lambda row: row.fecha_ingreso > '2016-01-01')\n",
    "#        .takeSample(false, 10) \n",
    "# obtiene 10 alumnos al azar que hayan ingresado despues del 2016-01-01 a la carrera 10.\n",
    "alumnosRDD.filter(lambda row: row[4] == '10') \\ # Toma los alumnos solo de la carrera 10.\n",
    "            .filter(lambda row: row[3] > '2016-01-01') \\ # Toma los alumnos que ingresarion despues del 2016-01-01.\n",
    "            .takeSample(False, 10) \\ # Toma al azar 10 registros.\n",
    "\n",
    "# RTA: Obtiene 10 alumnos al azar que hayan ingresado despues del 2016-01-01 a la carrera 10.\n",
    "# Conclusión: VERDADERO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resuelvo el punto c).\n",
    "# rdd.filter(lambda row: row.codigo_carrera == '10')\n",
    "#        .filter(lambda row: row.fecha_ingreso > '2016-01-01')\n",
    "#        .takeSample(false, 10) obtiene los 10 alumnos que se inscribieron en el año 2016 primeros a la carrera 10.\n",
    "\n",
    "# RTA: Es igual al caso b). No obtiene los alumnos que se inscribieron en 2016.\n",
    "# Conclusión: FALSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resuelvo el punto d).\n",
    "# rdd.map(lambda row: (row.fecha_ingreso.year, 1))\n",
    "#        .reduceByKey(lambda a, b: a+b).collect() \n",
    "# obtiene la cantidad de inscriptos por día.\n",
    "\n",
    "# RTA: Obtiene la cantidad de ingresos para los diferentes años.\n",
    "# Conclusión: FALSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  \tArchivo.txt:\n",
    "\n",
    "## datos spark guia\n",
    "## spark hadoop \n",
    "## guia datos spark gradience\n",
    "\n",
    "## Corremos:\n",
    "\n",
    "## sc.textFile(\"Archivo.txt\")\n",
    "## .flatMap(lambda a: a.split())\n",
    "## .map(lambda a: (a, 1))\n",
    "## .reduceByKey(lambda a, b: a+b)\n",
    "## .takeOrdered(3, key = lambda x: -x[1])\n",
    "\n",
    "### Cual de las respuestas es correcta?\n",
    "###         a) \tCon flatMap obtengo un registro para cada letra del archivo.\n",
    "### \t\tb) \tEl takeOrdered obtiene las 3 primeras palabras ordenadas alfabeticamente.\n",
    "### \t\tc) \tEl resultado son las 3 palabras más largas.\n",
    "### \t\td) \tEl reduceByKey suma la apariciones de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark', 3), (u'datos', 2), (u'guia', 2)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corro la instruccion.\n",
    "sc.textFile(\"Data/Archivo.txt\").flatMap(lambda a: a.split()).map(lambda a: (a, 1)).reduceByKey(lambda a, b: a+b).takeOrdered(3, key = lambda x: -x[1])\n",
    "\n",
    "# Veo que me trae las 3 palabras con mas frecuencia como tupla: (palabra, frecuencia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a) Con flatMap obtengo un registro para cada letra del archivo.\n",
    "\n",
    "# RTA: FALSO - Obtengo un registro por cada palabra del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b) El takeOrdered obtiene las 3 primeras palabras ordenadas alfabeticamente.\n",
    "\n",
    "# RTA: FALSO - Devuelve los primeros 3 registros ordenados segun el criterio establecido por parametro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c) El resultado son las 3 palabras más largas.\n",
    "\n",
    "# RTA: FALSO - El resultado son las 3 palabras con mas frecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d) El reduceByKey suma la apariciones de cada palabra.\n",
    "\n",
    "# RTA: VERDADERO - Al ser la key las palabras y el value un 1, el reduce by key suma las apariciones de cada palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  \tDado el siguiente código de pyspark e ignorando la definicion de pairRDD -- print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect() print pairRDD.reduceByKey(add).collect()\n",
    "## Obtenemos la siguiente salida: [('a', 3), ('b', 1)] [('a', 3), ('b', 1)]\n",
    "## ¿Como debería estar definida pairRDD para obtener ese resultado?\t \n",
    "### \t\t a) \tpairRDD = [('a', 2), ('a', 1), ('b', 1)]\n",
    "### \t\t b) \tpairRDD = sc.parallelize([('b', 1), ('b', 1), ('b', 1), ('a', 1)]).repartition(12)\n",
    "### \t\t c) \tpairRDD = sc.parallelize([('a', 1), ('a', 1), ('a', 1), ('b', 1)]).repartition(12)\n",
    "### \t\t d) \tpairRDD = sc.parallelize([('b', 1), ('b', 1), ('b', 1), ('a', 1)]).repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'groupByKey'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-09ee0a7c0e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# a) pairRDD = [('a', 2), ('a', 1), ('b', 1)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpairRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mpairRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mpairRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'groupByKey'"
     ]
    }
   ],
   "source": [
    "# a) pairRDD = [('a', 2), ('a', 1), ('b', 1)]\n",
    "# Salida: [('a', 3), ('b', 1)] [('a', 3), ('b', 1)]\n",
    "pairRDD = [('a', 2), ('a', 1), ('b', 1)]\n",
    "print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect()\n",
    "print pairRDD.reduceByKey(add).collect()\n",
    "\n",
    "# RTA: FALSA - pairRDD NO es un RDD sino una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 3), ('a', 1)]\n",
      "[('b', 3), ('a', 1)]\n"
     ]
    }
   ],
   "source": [
    "# b) pairRDD = sc.parallelize([('b', 1), ('b', 1), ('b', 1), ('a', 1)]).repartition(12)\n",
    "# Salida: [('a', 3), ('b', 1)] [('a', 3), ('b', 1)]\n",
    "\n",
    "from operator import add \n",
    "\n",
    "pairRDD = sc.parallelize([('b', 1), ('b', 1), ('b', 1), ('a', 1)]).repartition(12)\n",
    "print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect()\n",
    "print pairRDD.reduceByKey(add).collect()\n",
    "\n",
    "# RTA: FALSA - No dio igual el output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 1), ('a', 3)]\n",
      "[('b', 1), ('a', 3)]\n"
     ]
    }
   ],
   "source": [
    "# c) pairRDD = sc.parallelize([('a', 1), ('a', 1), ('a', 1), ('b', 1)]).repartition(12)\n",
    "# Salida: [('a', 3), ('b', 1)] [('a', 3), ('b', 1)]\n",
    "\n",
    "pairRDD = sc.parallelize([('a', 1), ('a', 1), ('a', 1), ('b', 1)]).repartition(12)\n",
    "print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect()\n",
    "print pairRDD.reduceByKey(add).collect()\n",
    "\n",
    "# RTA: VERDADERO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 3), ('a', 1)]\n",
      "[('b', 3), ('a', 1)]\n"
     ]
    }
   ],
   "source": [
    "# d) pairRDD = sc.parallelize([('b', 1), ('b', 1), ('b', 1), ('a', 1)]).repartition(5)\n",
    "# Salida: [('a', 3), ('b', 1)] [('a', 3), ('b', 1)]\n",
    "\n",
    "pairRDD = sc.parallelize([('b', 1), ('b', 1), ('b', 1), ('a', 1)]).repartition(5)\n",
    "print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect()\n",
    "print pairRDD.reduceByKey(add).collect()\n",
    "\n",
    "# RTA: FALSA - No dio igual el output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  \tComo framework de cluster computing, Apache Spark provee una abstracción llamada RDDs (Resilent Distributed Datasets). De los siguientes opciones, solo una es correcta en relación los RDDs.\t \n",
    "### \t\t a) \tSon particularmente lentos para tareas de Interactive Mining por las operaciones de entrada-salida que requieren.\n",
    "### \t\t b) \tSon reconstruidos automáticamente por Spark frente a un fallo en una maquina del cluster o si un cierto job se encuentra funcionando lento.\n",
    "### \t\t c) \tLos RDDs NO son inmutables, de hecho todas las operaciones que se aplican sobre los mismos afectan los datos que tenían originalmente.\n",
    "### \t\t d) \tSolo soportan la transformación map y la acción reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RTA: La respuesta b) es la correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  \tTenemos un RDD con números enteros llamado intergerListRDD al cual le aplicamos las siguientes acciones. \n",
    "## integersListRDD.reduce(lambda a, b: a - b) integersListRDD.repartition(6).reduce(lambda a, b: a - b) \n",
    "## Que podemos decir los resultados de la aplicar el reduce por un lado y de aplicar del repartition y el reduce.\t \n",
    "### \t\t a) \tLos resultados seran iguales dado que la operacion de resta es no conmutativa y asociativa.\n",
    "### \t\t b) \tLos resultados serán iguales dado que no hay que tener consideraciones especiales al aplicar una operacion de reduce sobre los datos.\n",
    "### \t\t c) \tLos resultados serán distintos, dado que el reduce es aplicado a nivel de partición y luego nuevamente para agregar resultados de particiones, por lo que se requiere que la función planteada de sea conmutativa y asociativa\n",
    "### \t\t d) \tLos resultados serán distintos dado que la operacion de repartition crea un nuevo RDD con el exacto número de particiones indicado (en este caso 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45769972\n",
      "33405152\n"
     ]
    }
   ],
   "source": [
    "# Creo el rdd.\n",
    "intergerListRDD = sc.parallelize(range(1,10000))\n",
    "print intergerListRDD.reduce(lambda a, b: a - b) \n",
    "print intergerListRDD.repartition(6).reduce(lambda a, b: a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a) Los resultados seran iguales dado que la operacion de resta es no conmutativa y asociativa.\n",
    "\n",
    "# RTA: FALSO - Los resultados son distintos debido a que la operacion de resta no es asociativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b) Los resultados serán iguales dado que no hay que tener consideraciones especiales al aplicar una \n",
    "#    operacion de reduce sobre los datos.\n",
    "\n",
    "# RTA: FALSO - La operacion tiene que ser asociativa y conmutativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c) Los resultados serán distintos, dado que el reduce es aplicado a nivel de partición y luego nuevamente \n",
    "#    para agregar resultados de particiones, por lo que se requiere que la función planteada de sea conmutativa \n",
    "#    y asociativa.\n",
    "\n",
    "# RTA: VERDADERO - La funcion aplicada al reduce tiene que ser conmutativa y asociativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d) Los resultados serán distintos dado que la operacion de repartition crea un nuevo RDD con el exacto \n",
    "#    número de particiones indicado (en este caso 6).\n",
    "\n",
    "# RTA: FALSA - No se crean nuevos RDDs, simplemente se distribuye sus datos en diferentes particiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  \tArchivo.txt:\n",
    "\n",
    "## datos spark guia\n",
    "## spark hadoop \n",
    "## guia datos spark gradience\n",
    "\n",
    "## Corremos:\n",
    "\n",
    "sc.textFile(\"Archivo.txt\")\n",
    ".flatMap(lambda a: a.split())\n",
    ".map(lambda a: (a, 1))\n",
    ".reduceByKey(lambda a, b: a+b)\n",
    ".takeOrdered(3, key = lambda x: -x[1])\n",
    "\n",
    "### Cual de las respuestas es correcta?\n",
    " \n",
    "#### \t\t a) \tNo es posible hacer un takeOrdered porque hay un reduceByKey antes.\n",
    "#### \t\t b) \tEl resultado es [('spark', 3), ('datos', 2), ('guia', 2)]\n",
    "#### \t\t c) \tMap no puede funcionar desde un archivo.\n",
    "#### \t\t d) \tEl resultado son las 3 últimas palabras ordenadas alfabeticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark', 3), (u'datos', 2), (u'guia', 2)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corro la instruccion.\n",
    "sc.textFile(\"Data/Archivo.txt\").flatMap(lambda a: a.split()).map(lambda a: (a, 1)).reduceByKey(lambda a, b: a+b).takeOrdered(3, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a) No es posible hacer un takeOrdered porque hay un reduceByKey antes.\n",
    "\n",
    "# RTA: FALSO - El take ordered se puede hacer sobre cualquier rdd sobre el cual tengamos alguna propiedad para comparar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b) El resultado es [('spark', 3), ('datos', 2), ('guia', 2)]\n",
    "\n",
    "# RTA: Verd - Parece verdadero salvo pos las \"u\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c) Map no puede funcionar desde un archivo.\n",
    "\n",
    "# RTA: FALSO - No se por que!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d) El resultado son las 3 últimas palabras ordenadas alfabeticamente.\n",
    "\n",
    "# RTA: FALSO - El resultado son las 3 palabras con mas frecuencia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
